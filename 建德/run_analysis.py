#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å»ºå¾·åƒåœ¾ç„šçƒ§é¢„è­¦æŠ¥è­¦åˆ†æ - ç›´æ¥è¿è¡Œè„šæœ¬
"""

import os
import sys
import glob
import pandas as pd
from datetime import datetime
from pathlib import Path

# å¯¼å…¥ç®—æ³•
try:
    from shishi_data_yujing_gz import WasteIncinerationWarningSystemJiande
    print("âœ… æˆåŠŸå¯¼å…¥å»ºå¾·é¢„è­¦æŠ¥è­¦ç®—æ³•")
except ImportError as e:
    print(f"âŒ å¯¼å…¥ç®—æ³•å¤±è´¥: {e}")
    sys.exit(1)

def find_process_files(data_dir):
    """æŸ¥æ‰¾æ‰€æœ‰_process.xlsxæ–‡ä»¶"""
    pattern = os.path.join(data_dir, "**", "*_process.xlsx")
    files = glob.glob(pattern, recursive=True)
    return sorted(files)

def analyze_single_file(file_path, warning_system, output_dir):
    """åˆ†æå•ä¸ªæ–‡ä»¶"""
    print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {os.path.basename(file_path)}")

    try:
        start_time = datetime.now()
        result_df = warning_system.process_data(file_path, output_dir)
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        if not result_df.empty:
            print(f"   âœ… æ£€æµ‹åˆ° {len(result_df)} æ¡äº‹ä»¶ï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return len(result_df), result_df
        else:
            print(f"   âœ… æ— äº‹ä»¶ï¼Œè€—æ—¶: {duration:.2f}ç§’")
            return 0, result_df

    except Exception as e:
        print(f"   âŒ åˆ†æå¤±è´¥: {e}")
        return 0, None

def generate_integrated_report(all_results, output_dir, total_files, successful_files,
                             failed_files, total_events, total_duration):
    """ç”Ÿæˆé›†æˆæŠ¥å‘Š"""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

    # 1. ç”Ÿæˆé›†æˆçš„é¢„è­¦æŠ¥è­¦äº‹ä»¶Excelæ–‡ä»¶
    if all_results:
        print("   ğŸ“Š åˆå¹¶æ‰€æœ‰é¢„è­¦æŠ¥è­¦äº‹ä»¶...")
        all_events = []

        for file_path, result_df in all_results:
            # æ·»åŠ æ•°æ®æ¥æºåˆ—
            result_df_copy = result_df.copy()
            result_df_copy['æ•°æ®æ¥æº'] = os.path.basename(file_path)

            # æå–æ—¥æœŸä¿¡æ¯
            file_name = os.path.basename(file_path)
            if '_process.xlsx' in file_name:
                date_part = file_name.replace('_process.xlsx', '')
                result_df_copy['æ•°æ®æ—¥æœŸ'] = date_part

            all_events.append(result_df_copy)

        # åˆå¹¶æ‰€æœ‰äº‹ä»¶
        integrated_df = pd.concat(all_events, ignore_index=True)

        # æŒ‰æ—¶é—´æ’åº
        integrated_df = integrated_df.sort_values('æ—¶é—´')

        # ä¿å­˜é›†æˆExcelæ–‡ä»¶
        excel_file = os.path.join(output_dir, f"å»ºå¾·é¢„è­¦æŠ¥è­¦é›†æˆæŠ¥å‘Š_{timestamp}.xlsx")

        # åˆ›å»ºå¤šä¸ªå·¥ä½œè¡¨
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # ä¸»æŠ¥å‘Šå·¥ä½œè¡¨
            integrated_df.to_excel(writer, sheet_name='é¢„è­¦æŠ¥è­¦äº‹ä»¶', index=False)

            # ç»Ÿè®¡æ±‡æ€»å·¥ä½œè¡¨
            create_summary_sheet(writer, integrated_df, all_results, total_files,
                                successful_files, failed_files, total_events, total_duration)

            # æŒ‰ç±»å‹åˆ†ç»„å·¥ä½œè¡¨
            create_type_analysis_sheet(writer, integrated_df)

            # æŒ‰æ—¥æœŸåˆ†ç»„å·¥ä½œè¡¨
            create_date_analysis_sheet(writer, integrated_df)

        print(f"   âœ… é›†æˆExcelæŠ¥å‘Š: {excel_file}")

        # ä¿å­˜é›†æˆCSVæ–‡ä»¶
        csv_file = os.path.join(output_dir, f"å»ºå¾·é¢„è­¦æŠ¥è­¦é›†æˆæŠ¥å‘Š_{timestamp}.csv")
        integrated_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        print(f"   âœ… é›†æˆCSVæŠ¥å‘Š: {csv_file}")

    # 2. ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡æ–‡ä»¶
    stats_file = os.path.join(output_dir, f"å»ºå¾·æ‰¹é‡åˆ†æç»Ÿè®¡æŠ¥å‘Š_{timestamp}.txt")
    with open(stats_file, 'w', encoding='utf-8') as f:
        f.write("å»ºå¾·åƒåœ¾ç„šçƒ§æ‰¹é‡åˆ†æç»Ÿè®¡æŠ¥å‘Š\n")
        f.write("=" * 50 + "\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"åˆ†æè€—æ—¶: {total_duration:.2f} ç§’\n\n")

        f.write("å¤„ç†æ¦‚å†µ:\n")
        f.write(f"  æ€»æ–‡ä»¶æ•°: {total_files}\n")
        f.write(f"  æˆåŠŸå¤„ç†: {successful_files}\n")
        f.write(f"  å¤„ç†å¤±è´¥: {failed_files}\n")
        f.write(f"  æ€»äº‹ä»¶æ•°: {total_events}\n\n")

        if all_results:
            f.write("æ–‡ä»¶è¯¦æƒ…:\n")
            for file_path, result_df in all_results:
                file_name = os.path.basename(file_path)
                f.write(f"  {file_name}: {len(result_df)} æ¡äº‹ä»¶\n")

                # äº‹ä»¶ç±»å‹ç»Ÿè®¡
                type_stats = result_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'].value_counts()
                for event_type, count in type_stats.items():
                    f.write(f"    - {event_type}: {count} æ¡\n")
                f.write("\n")

    print(f"   âœ… ç»Ÿè®¡æŠ¥å‘Š: {stats_file}")

def create_summary_sheet(writer, integrated_df, all_results, total_files,
                        successful_files, failed_files, total_events, total_duration):
    """åˆ›å»ºæ±‡æ€»ç»Ÿè®¡å·¥ä½œè¡¨"""
    summary_data = []

    # åŸºæœ¬ç»Ÿè®¡
    summary_data.append(['ç»Ÿè®¡é¡¹ç›®', 'æ•°å€¼'])
    summary_data.append(['å¤„ç†æ–‡ä»¶æ€»æ•°', total_files])
    summary_data.append(['æˆåŠŸå¤„ç†æ–‡ä»¶', successful_files])
    summary_data.append(['å¤„ç†å¤±è´¥æ–‡ä»¶', failed_files])
    summary_data.append(['æ€»äº‹ä»¶æ•°', total_events])
    summary_data.append(['åˆ†æè€—æ—¶(ç§’)', f"{total_duration:.2f}"])
    summary_data.append(['', ''])

    # äº‹ä»¶ç±»å‹ç»Ÿè®¡
    summary_data.append(['äº‹ä»¶ç±»å‹ç»Ÿè®¡', ''])
    if not integrated_df.empty:
        type_stats = integrated_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'].value_counts()
        for event_type, count in type_stats.items():
            summary_data.append([event_type, count])

    summary_data.append(['', ''])

    # äº‹ä»¶è¯¦ç»†ç»Ÿè®¡
    summary_data.append(['äº‹ä»¶è¯¦ç»†ç»Ÿè®¡', ''])
    if not integrated_df.empty:
        event_stats = integrated_df['é¢„è­¦/æŠ¥è­¦äº‹ä»¶'].value_counts()
        for event, count in event_stats.items():
            summary_data.append([event, count])

    summary_df = pd.DataFrame(summary_data)
    summary_df.to_excel(writer, sheet_name='ç»Ÿè®¡æ±‡æ€»', index=False, header=False)

def create_type_analysis_sheet(writer, integrated_df):
    """åˆ›å»ºæŒ‰ç±»å‹åˆ†æå·¥ä½œè¡¨"""
    if integrated_df.empty:
        return

    type_analysis = []
    type_analysis.append(['é¢„è­¦/æŠ¥è­¦ç±»å‹', 'äº‹ä»¶åç§°', 'æ•°é‡', 'å æ¯”(%)'])

    type_stats = integrated_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'].value_counts()
    total_count = len(integrated_df)

    for event_type in type_stats.index:
        type_df = integrated_df[integrated_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'] == event_type]
        event_stats = type_df['é¢„è­¦/æŠ¥è­¦äº‹ä»¶'].value_counts()

        for event_name, count in event_stats.items():
            percentage = (count / total_count) * 100
            type_analysis.append([event_type, event_name, count, f"{percentage:.2f}"])

    type_df = pd.DataFrame(type_analysis[1:], columns=type_analysis[0])
    type_df.to_excel(writer, sheet_name='ç±»å‹åˆ†æ', index=False)

def create_date_analysis_sheet(writer, integrated_df):
    """åˆ›å»ºæŒ‰æ—¥æœŸåˆ†æå·¥ä½œè¡¨"""
    if integrated_df.empty or 'æ•°æ®æ—¥æœŸ' not in integrated_df.columns:
        return

    date_analysis = []
    date_analysis.append(['æ•°æ®æ—¥æœŸ', 'é¢„è­¦æ•°é‡', 'æŠ¥è­¦æ•°é‡', 'æ€»æ•°é‡'])

    date_stats = integrated_df.groupby('æ•°æ®æ—¥æœŸ').agg({
        'é¢„è­¦/æŠ¥è­¦ç±»å‹': lambda x: (x == 'é¢„è­¦').sum(),
        'é¢„è­¦/æŠ¥è­¦äº‹ä»¶': 'count'
    }).reset_index()

    date_stats.columns = ['æ•°æ®æ—¥æœŸ', 'é¢„è­¦æ•°é‡', 'æ€»æ•°é‡']
    date_stats['æŠ¥è­¦æ•°é‡'] = integrated_df.groupby('æ•°æ®æ—¥æœŸ')['é¢„è­¦/æŠ¥è­¦ç±»å‹'].apply(lambda x: (x == 'æŠ¥è­¦').sum()).values

    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
    date_stats = date_stats[['æ•°æ®æ—¥æœŸ', 'é¢„è­¦æ•°é‡', 'æŠ¥è­¦æ•°é‡', 'æ€»æ•°é‡']]

    date_stats.to_excel(writer, sheet_name='æ—¥æœŸåˆ†æ', index=False)

def batch_analysis():
    """æ‰¹é‡åˆ†ææ¨¡å¼"""
    print("=" * 80)
    print("ğŸ­ å»ºå¾·åƒåœ¾ç„šçƒ§é¢„è­¦æŠ¥è­¦åˆ†æç³»ç»Ÿ - æ‰¹é‡åˆ†ææ¨¡å¼")
    print("=" * 80)

    # é…ç½®è·¯å¾„
    data_dir = "å»ºå¾·/å»ºå¾·æ•°æ®"
    output_dir = "./å»ºå¾·/é¢„è­¦è¾“å‡º"

    print(f"ï¿½ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ï¿½ è¾“å‡ºç›®å½•: {output_dir}")

    # æŸ¥æ‰¾æ‰€æœ‰_process.xlsxæ–‡ä»¶
    print(f"\nğŸ” æœç´¢_process.xlsxæ–‡ä»¶...")
    process_files = find_process_files(data_dir)

    if not process_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•_process.xlsxæ–‡ä»¶")
        return

    print(f"âœ… æ‰¾åˆ° {len(process_files)} ä¸ªæ–‡ä»¶")

    # æŒ‰æœˆä»½åˆ†ç»„æ˜¾ç¤º
    files_by_month = {}
    for file_path in process_files:
        # æå–æœˆä»½ä¿¡æ¯
        path_parts = file_path.split(os.sep)
        month_info = "æœªçŸ¥æœˆä»½"
        for part in path_parts:
            if "æœˆ" in part:
                month_info = part
                break

        if month_info not in files_by_month:
            files_by_month[month_info] = []
        files_by_month[month_info].append(file_path)

    print(f"\nğŸ“‹ æ–‡ä»¶åˆ†å¸ƒ:")
    for month, files in files_by_month.items():
        print(f"   {month}: {len(files)} ä¸ªæ–‡ä»¶")

    # åˆ›å»ºé¢„è­¦ç³»ç»Ÿå®ä¾‹
    print(f"\nğŸ”§ åˆ›å»ºé¢„è­¦ç³»ç»Ÿå®ä¾‹...")
    warning_system = WasteIncinerationWarningSystemJiande()

    # æ‰¹é‡å¤„ç†
    print(f"\nğŸš€ å¼€å§‹æ‰¹é‡åˆ†æ...")
    total_start_time = datetime.now()

    total_files = len(process_files)
    total_events = 0
    successful_files = 0
    failed_files = 0
    all_results = []

    for i, file_path in enumerate(process_files, 1):
        print(f"\n[{i}/{total_files}] å¤„ç†: {file_path}")

        event_count, result_df = analyze_single_file(file_path, warning_system, output_dir)

        if result_df is not None:
            successful_files += 1
            total_events += event_count
            if not result_df.empty:
                all_results.append((file_path, result_df))
        else:
            failed_files += 1

    total_end_time = datetime.now()
    total_duration = (total_end_time - total_start_time).total_seconds()

    # ç”Ÿæˆé›†æˆæŠ¥å‘Š
    print(f"\nğŸ“‹ ç”Ÿæˆé›†æˆæŠ¥å‘Š...")
    generate_integrated_report(all_results, output_dir, total_files, successful_files,
                             failed_files, total_events, total_duration)

    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    print(f"\n" + "=" * 80)
    print("ğŸ“Š æ‰¹é‡åˆ†ææ±‡æ€»ç»“æœ")
    print("=" * 80)
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶æ€»æ•°: {total_files}")
    print(f"âœ… æˆåŠŸå¤„ç†: {successful_files}")
    print(f"âŒ å¤„ç†å¤±è´¥: {failed_files}")
    print(f"ğŸ¯ æ€»äº‹ä»¶æ•°: {total_events}")
    print(f"â±ï¸ æ€»è€—æ—¶: {total_duration:.2f} ç§’")
    print(f"ğŸ“‚ æŠ¥å‘Šä¿å­˜ç›®å½•: {output_dir}")

    if all_results:
        print(f"\nğŸ“‹ æœ‰äº‹ä»¶çš„æ–‡ä»¶è¯¦æƒ…:")
        for file_path, result_df in all_results:
            file_name = os.path.basename(file_path)
            type_stats = result_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'].value_counts()
            stats_str = ", ".join([f"{t}:{c}æ¡" for t, c in type_stats.items()])
            print(f"   {file_name}: {len(result_df)}æ¡äº‹ä»¶ ({stats_str})")

    print(f"\nğŸ‰ æ‰¹é‡åˆ†æå®Œæˆï¼")

def single_analysis():
    """å•æ–‡ä»¶åˆ†ææ¨¡å¼"""
    print("=" * 80)
    print("ğŸ­ å»ºå¾·åƒåœ¾ç„šçƒ§é¢„è­¦æŠ¥è­¦åˆ†æç³»ç»Ÿ - å•æ–‡ä»¶åˆ†ææ¨¡å¼")
    print("=" * 80)

    # é…ç½®æ–‡ä»¶è·¯å¾„
    input_file = "å»ºå¾·/å»ºå¾·æ•°æ®/2025å¹´6æœˆ/6.1_process.xlsx"
    output_dir = "./å»ºå¾·/é¢„è­¦è¾“å‡º"

    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {input_file}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_file):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return

    # åˆ›å»ºé¢„è­¦ç³»ç»Ÿå®ä¾‹
    print(f"\nğŸ”§ åˆ›å»ºé¢„è­¦ç³»ç»Ÿå®ä¾‹...")
    warning_system = WasteIncinerationWarningSystemJiande()

    # å¼€å§‹åˆ†æ
    print(f"\nğŸš€ å¼€å§‹åˆ†ææ•°æ®...")
    start_time = datetime.now()

    try:
        # å¤„ç†æ•°æ®
        result_df = warning_system.process_data(input_file, output_dir)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        print(f"\nâ±ï¸ åˆ†æå®Œæˆï¼Œè€—æ—¶: {duration:.2f} ç§’")

        if not result_df.empty:
            print(f"\nğŸ¯ åˆ†æç»“æœ:")
            print(f"   æ€»è®¡æ£€æµ‹åˆ° {len(result_df)} æ¡é¢„è­¦æŠ¥è­¦äº‹ä»¶")

            # æ˜¾ç¤ºäº‹ä»¶ç±»å‹ç»Ÿè®¡
            type_stats = result_df['é¢„è­¦/æŠ¥è­¦ç±»å‹'].value_counts()
            print(f"\nğŸ“Š äº‹ä»¶ç±»å‹ç»Ÿè®¡:")
            for event_type, count in type_stats.items():
                print(f"   {event_type}: {count} æ¡")

            # æ˜¾ç¤ºäº‹ä»¶è¯¦ç»†ç»Ÿè®¡
            event_stats = result_df['é¢„è­¦/æŠ¥è­¦äº‹ä»¶'].value_counts()
            print(f"\nğŸ“‹ äº‹ä»¶è¯¦ç»†ç»Ÿè®¡:")
            for event, count in event_stats.items():
                print(f"   {event}: {count} æ¡")

            # æ˜¾ç¤ºå‰å‡ æ¡äº‹ä»¶
            print(f"\nğŸ” å‰5æ¡äº‹ä»¶:")
            for i, (_, row) in enumerate(result_df.head().iterrows()):
                print(f"   {i+1}. {row['æ—¶é—´']} - {row['é¢„è­¦/æŠ¥è­¦äº‹ä»¶']} ({row['é¢„è­¦/æŠ¥è­¦ç±»å‹']})")

            print(f"\nğŸ“ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_dir}")
        else:
            print(f"\nâœ… åˆ†æå®Œæˆï¼Œæœªå‘ç°é¢„è­¦æŠ¥è­¦äº‹ä»¶")
            print("   æ•°æ®æ­£å¸¸ï¼Œæ— éœ€å…³æ³¨")

    except Exception as e:
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return

    print(f"\n" + "=" * 80)
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print("=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å»ºå¾·åƒåœ¾ç„šçƒ§é¢„è­¦æŠ¥è­¦åˆ†æç³»ç»Ÿ")
    print("è¯·é€‰æ‹©åˆ†ææ¨¡å¼:")
    print("1. å•æ–‡ä»¶åˆ†æ")
    print("2. æ‰¹é‡åˆ†æ")

    # è‡ªåŠ¨é€‰æ‹©æ‰¹é‡åˆ†ææ¨¡å¼
    choice = "2"  # é»˜è®¤æ‰¹é‡åˆ†æ

    if choice == "1":
        single_analysis()
    elif choice == "2":
        batch_analysis()
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œé»˜è®¤ä½¿ç”¨æ‰¹é‡åˆ†ææ¨¡å¼")
        batch_analysis()

if __name__ == "__main__":
    main()
